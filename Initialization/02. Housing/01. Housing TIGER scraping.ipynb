{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing TIGER Scraping\n",
    "\n",
    "This script is designed to scrape census data from the TIGER/Line Shapefiles provided by the United States Census Bureau, specifically the specified year and state. You can find the data source [here](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-line-file.html).\n",
    "\n",
    "### Runtime\n",
    "\n",
    "Approximately 3 minutes for Washington State.  Runtime may differ by state.\n",
    "\n",
    "### High Level Overview\n",
    "This script downloads, processes, and merges geographic data from the US Census Bureau's Topologically Integrated Geographic Encoding and Referencing (TIGER) system. The script is configurable for a specific US state and will handle data for all counties within that state. The output is a comprehensive dataset containing selected geographic features and their attributes for the specified state.\n",
    "\n",
    "### Detailed Overview\n",
    "The script is set to scrape data for a specified state (in this case, \"Washington\") and all of its counties. For each county, it downloads three different types of data files: 'ARRF', 'ARFNRF', 'FNRF', which respectively create 'addr', 'addrfn', and 'featnames' files.\n",
    "\n",
    "These files are then combined together. The 'addr' and 'addrfn' files are joined on the 'ARID' field, and then the 'featnames' files are joined on the 'LINEARID' field. After joining, the following fields are extracted: 'FULLNAME','FROMHN','TOHN','SIDE','ZIP','PLUS4','MTFCC'. All of these operations are performed for each county.\n",
    "\n",
    "At the end of the script, all the data is merged together to provide the required information for all the counties within the state.\n",
    "\n",
    "A list of counties for the given state is also generated at the start of the script and used for a quality check at the end to verify that data has been extracted for all counties.\n",
    "\n",
    "The script also performs a check to ensure the required columns exist in the downloaded data files before performing any merging operations. If they do not exist, a warning message is printed.\n",
    "\n",
    "### Notes\n",
    "* The TIGER/Line Shapefiles website mentions that users should not download large amounts of data during their peak usage hours of 8 AM to 4 PM Eastern time.\n",
    "* The columns specified might not be present in all the shapefiles depending on the state and county.\n",
    "* The script downloads each TIGER file separately for each county. Depending on the amount of data you plan to download, you may want to consider downloading the data in bulk and then extracting the files for each county.\n",
    "* The relative year's TIGER files might not be available at the given URL, as the URL structure can change based on the year and the version of the dataset. You need to adjust the URL according to the correct path.\n",
    "\n",
    "--------- \n",
    "\n",
    "### About\n",
    "\n",
    "<p>Author: PJ Gibson</p>\n",
    "<p>Created Date: 2023-06-30</p>\n",
    "<p>Contact: peter.gibson@doh.wa.gov</p>\n",
    "<p>Assistance in the generation of this script was provided by GPT-4, a model developed by OpenAI.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from simpledbf import Dbf5\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Specify the state you want\n",
    "state = \"Washington\"\n",
    "fips = \"53\"\n",
    "\n",
    "year = \"2019\"\n",
    "\n",
    "# Create a directory for the downloaded files if it does not exist\n",
    "folder_name = f'TIGERfiles_{state}'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Load the key from the pickle file\n",
    "with open('secrets.pkl', 'rb') as f:\n",
    "    api_key = pickle.load(f)\n",
    "\n",
    "print(\"API key loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US Census FTP server\n",
    "base_url = f'https://www2.census.gov/geo/tiger/TIGER{year}'\n",
    "\n",
    "params = {\"key\": api_key}\n",
    "\n",
    "# List of counties\n",
    "counties_url = f'{base_url}/COUNTY/tl_{year}_us_county.zip'\n",
    "counties_file = f'{folder_name}/tl_{year}_us_county.zip'\n",
    "\n",
    "# Download the file to zip\n",
    "response = requests.get(counties_url, params=params)\n",
    "with open(counties_file, 'wb') as output:\n",
    "    output.write(response.content)\n",
    "\n",
    "# Unzip the county file\n",
    "with zipfile.ZipFile(counties_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(f'{folder_name}/')\n",
    "\n",
    "# Read in which counties belong to our state of interest\n",
    "counties = gpd.read_file(f'{folder_name}/tl_{year}_us_county.shp')\n",
    "state_counties = counties[counties['STATEFP'] == fips]\n",
    "\n",
    "# Define the dataframe that will guide our data extraction by county and check for quality completeness\n",
    "counties_list = state_counties['NAME'].to_frame()\n",
    "\n",
    "# Begin extracting, combining, and saving data for each county.\n",
    "for i in tqdm(np.arange(0,len(state_counties))):\n",
    "\n",
    "    # Define row iteration within our for-loop\n",
    "    row = state_counties.iloc[i]\n",
    "\n",
    "    # Define the county of interest. Used primarily for file naming\n",
    "    county = row['NAME']\n",
    "\n",
    "    # For each file type below, we will append the output dataframe to this empty list\n",
    "    files_to_combine = []\n",
    "\n",
    "    for file_type in ['addr', 'featnames', 'addrfn']:\n",
    "        file_url = f'{base_url}/{file_type.upper()}/tl_{year}_{row[\"GEOID\"]}_{file_type}.zip'\n",
    "        file_name = f'{folder_name}/tl_{year}_{row[\"GEOID\"]}_{file_type}.zip'\n",
    "\n",
    "        # Download the file\n",
    "        response = requests.get(file_url, params=params)\n",
    "        with open(file_name, 'wb') as output:\n",
    "            output.write(response.content)\n",
    "\n",
    "        # Unzip the file\n",
    "        with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(f'{folder_name}/{county}_{file_type}/')\n",
    "\n",
    "        # Read in the .dbf file, one of the unzipped contents\n",
    "        dbf = Dbf5(f'{folder_name}/{county}_{file_type}/tl_{year}_{row[\"GEOID\"]}_{file_type}.dbf')\n",
    "\n",
    "        # Convert to pandas dataframe\n",
    "        df = dbf.to_dataframe()\n",
    "\n",
    "        # Append to dataframe to combine outside of for loop\n",
    "        files_to_combine.append(df)\n",
    "\n",
    "    # Merge the files\n",
    "    merged = pd.merge(files_to_combine[0].drop('MTFCC',axis=1), files_to_combine[2], on='ARID', how='outer')\n",
    "    merged = pd.merge(merged, files_to_combine[1], on='LINEARID', how='outer')\n",
    "\n",
    "    # Extract the required fields and drop duplicates\n",
    "    merged = merged[['FULLNAME', 'FROMHN', 'TOHN', 'SIDE', 'ZIP', 'PLUS4', 'MTFCC']].drop_duplicates()\n",
    "\n",
    "    # Define fips value\n",
    "    merged['COUNTY_FIPS'] = int(row[\"GEOID\"])\n",
    "\n",
    "    # Define county name\n",
    "    merged['COUNTY_NAME'] = county\n",
    "\n",
    "    # Save the result\n",
    "    merged.to_csv(f'{folder_name}/{county}_combined.csv', index=False)\n",
    "\n",
    "# Merge all county data\n",
    "all_counties = pd.concat([pd.read_csv(f'{folder_name}/{county}_combined.csv') for county in state_counties['NAME']])\n",
    "\n",
    "# We only will allow people to live on roads designed for housing (non-highways, bike paths, 4x4 access only ect...)\n",
    "###### See https://www2.census.gov/geo/pdfs/maps-data/data/tiger/tgrshp2019/TGRSHP2019_TechDoc.pdf (end of paper) for detail on MTFCC options.\n",
    "all_counties = all_counties.query('MTFCC == \"S1400\"')\n",
    "\n",
    "# Save the result\n",
    "all_counties.to_csv(f'../../SupportingDocs/Housing/03_Complete/TIGER_state_streets.csv', index=False)\n",
    "\n",
    "# Quality check: Check if all counties' data have been extracted\n",
    "extracted_counties = all_counties['COUNTY_NAME'].unique().tolist()\n",
    "counties_list['Extracted'] = counties_list['NAME'].isin(extracted_counties)\n",
    "missing_counties = counties_list[~counties_list['Extracted']]['NAME'].tolist()\n",
    "if missing_counties:\n",
    "    print(f'WARNING: Data for the following counties were not extracted: {missing_counties}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

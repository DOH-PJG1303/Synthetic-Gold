{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Firstnames\n",
    "\n",
    "The purpose of this script is to wrangle data on firstnames.\n",
    "\n",
    "### Datasource #1 \n",
    "captured from the census via API, [webpage linked here](https://www.ssa.gov/oact/babynames/limits.html).\n",
    "\n",
    "### Datasource #2\n",
    "Captured from a published peice of literature that captures approximate race breakdowns of 4,250 first names based on mortgage datasets.\n",
    "See [link here](https://www.nature.com/articles/sdata201825) for information on the article/study.\n",
    "See [link here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TYJKEZ) for information on where the data was downloaded from.\n",
    "\n",
    "\n",
    "\n",
    "----------------------\n",
    "\n",
    "<p>Author: PJ Gibson</p>\n",
    "<p>Date: 2023-01-10</p>\n",
    "<p>Updated Date: 2023-07-26</p>\n",
    "<p>Contact: peter.gibson@doh.wa.gov</p>\n",
    "<p>Other Contact: pjgibson25@gmail.com</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import libraries, functions, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_abbreviation = 'WA'\n",
    "state_name = 'Washington'\n",
    "state_fips = '53'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_abbreviations = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Guam': 'GU',\n",
    "    'American Samoa': 'AS',\n",
    "    'U.S. Virgin Islands': 'VI',\n",
    "    'Northern Mariana Islands': 'MP'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. API pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URLs of the zip files\n",
    "urls = [\n",
    "    \"https://www.ssa.gov/oact/babynames/names.zip\",\n",
    "    \"https://www.ssa.gov/oact/babynames/state/namesbystate.zip\",\n",
    "    \"https://www.ssa.gov/oact/babynames/territory/namesbyterritory.zip\",\n",
    "]\n",
    "\n",
    "# For each file download...\n",
    "for url in urls:\n",
    "\n",
    "    # Get the file name by splitting the url and picking up the last string\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    \n",
    "    # Create a directory name based on the file name\n",
    "    directory_name = file_name.replace('.zip', '')\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    r = requests.get(url, stream=True)\n",
    "\n",
    "    # Save the response content as a zip file\n",
    "    with open(file_name, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=128):\n",
    "            fd.write(chunk)\n",
    "\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "        # Create a new directory with the same name as the zip file (without the extension)\n",
    "        os.makedirs(f'../../SupportingDocs/Names/01_Raw/{directory_name}', exist_ok=True)\n",
    "        # Extract all the contents of zip file in current directory\n",
    "        zip_ref.extractall(f'../../SupportingDocs/Names/01_Raw/{directory_name}')\n",
    "    \n",
    "    # Delete the zip file\n",
    "    os.remove(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vital Stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vital_stats = pd.read_csv('../../SupportingDocs/Births/03_Complete/VitalStats_byYear_byState.csv').query(f'State == \"{state_name}\"')[['Year','Births']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 National name data by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "\n",
    "for curyear in range(1880,2022):\n",
    "\n",
    "    df = pd.read_csv(f'../../SupportingDocs/Names/01_Raw/names/yob{curyear}.txt', header=None)\n",
    "    df.columns = ['Name','Sex','Count']\n",
    "    df['Year'] = curyear\n",
    "    df['Name'] = df['Name'].str.upper()\n",
    "    df['id_1'] = df['Sex'] + ' | ' + df['Name'] + ' | ' + df['Year'].astype(str)\n",
    "    df['id_2'] = df['Sex'] + ' | ' + df['Name']\n",
    "    output.append(df)\n",
    "    \n",
    "df_national = pd.concat(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 State name data by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_state = ['State','Sex','Year','Name','Count']\n",
    "df_state_names = pd.read_csv(f'../../SupportingDocs/Names/01_Raw/namesbystate/STATE.{state_abbreviation}.TXT', names=column_names_state)\n",
    "\n",
    "df_state_names['Name'] = df_state_names['Name'].str.upper()\n",
    "df_state_names['id_1'] = df_state_names['Sex'] + ' | ' + df_state_names['Name'] + ' | ' + df_state_names['Year'].astype(str)\n",
    "df_state_names['id_2'] = df_state_names['Sex'] + ' | ' + df_state_names['Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Wrangling\n",
    "\n",
    "We have a count for each name in each state for each year just through the census data.\n",
    "Which is awesome!\n",
    "Unfortunately, they mask any names that have less than 5 occurrences.\n",
    "We want to include these rarer names, which in my experience can often come from under-represented populations.\n",
    "At the very least, their names are under-represented...\n",
    "\n",
    "Here are some of the knowns:\n",
    "* We have the number of estimated births for a given state each year (Vital Stats)\n",
    "* We have counts of newborn names for a given state each year *excluding* for names that occur less than 5 times in that state & year.\n",
    "* We have counts of newborn names at the national level for each year *excluding* names that occur less than 5 times in a given year nationally.\n",
    "\n",
    "So when we perform the math:\n",
    "\n",
    "$$ NumBirths_{year} - \\sum{StateNameCounts_{year}} \\approx NumMaskedNames $$\n",
    "\n",
    "We want to generate data for the \"Masked Names\".\n",
    "To do this, we'll pull from the national data.\n",
    "See example below:\n",
    "\n",
    "> consider for the year 1990, there were 3 males in Washington state with the name \"Xenon\".  The SSA file `namesbystate/STATE.WA.TXT` would exclude that name in the data (for the year 1990) because there are fewer than 5 occurrences.  For this example however, on a national scale there are 30 other males named \"Xenon\" in other states.  Hence there are a total of 30 + 3 = 33 males named \"Xenon\" in the United States.  Because 33 occurrences surpasses the masking-threshold, the name \"Xenon\" would appear in the SSA file `names/yob1990.txt`.  Thus, we can use national data to infer small numbers unrepresented in the state-level data.\n",
    "\n",
    "We'll use this sort of logic to attempt to fill in the number of masked names.\n",
    "Below is an example of a generated \"Masked Name\" record:\n",
    "\n",
    "| State | Sex | Year | Name | Count |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| WA | F | 1990 | Yolanda | 4 |\n",
    "\n",
    "\n",
    "### Pool #1\n",
    "\n",
    "Any name/sex combinations not found in our state file that is found in the national file for the same year can be considered as **Pool #1**, a pool to pull for when generating \"Masked Names\".\n",
    "The count of any single \"Masked Name\" for a given state in a given year must fall between 1-4 times (<5 masking threshold).\n",
    "We pull from **Pool #1** using a custom function.\n",
    "This custom function applies the probabilities of a record in *Pool #1* belonging to the masked records in comparison to every other record in the pool.\n",
    "However, a record can be chosen a maximum of 4 times.\n",
    "If all records have been chosen 4 times and there are still records to mask...(see below)\n",
    "\n",
    "\n",
    "### Pool #2\n",
    "\n",
    "So we've used all of the reserves in Pool #1 four times and still haven't generated the proper number of masked records.\n",
    "Now, we use all national name data available for all years that is previously unseen for the given year.\n",
    "This group of exceedingly rare records will comprise **Pool #2**.\n",
    "Using a similar logic, we'll loop through this pool and assign a count of masked records to each name within the pool.\n",
    "\n",
    "If we still haven't reached the NumMaskedRecords by the end of this section, we end our efforts to generate more masked records and know that we tried our best to account for less common names.\n",
    "The script will run perfectly fine without reaching $NumMaskedRecords=0$.\n",
    "The primary purpose is to account for rarer situations to account for underrepresented individuals.\n",
    "\n",
    "---\n",
    "\n",
    "Side note, there's a chance that the names used in pool #1 or pool #2 have more representation of males or females.\n",
    "Since we're using their raw count to determine likelyhood of being added as a \"masked record\", there's a chance that we could inadvertently create more \"masked records\" of one specific gender as opposed to the other.\n",
    "While you could argue that you could put more coding logic in place to combat this, this is more of a problem with the SSA data collection.\n",
    "And if you still have a quarrel with it, [click here](https://www.youtube.com/watch?v=Vim4ZKuNm6k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Define function for generating names\n",
    "\n",
    "This function serves as the way that we pull names from pool #1 and pool #2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_names(df, n, id_column, year, state):\n",
    "\n",
    "    # In the instance where the number of desired records for masking is > 4 times the length of df...\n",
    "    if (n > len(df)*4):\n",
    "        \n",
    "        # Our second output will return false\n",
    "        output2 = False\n",
    "\n",
    "        # We redefine n as to not break the function\n",
    "        n = len(df)*4        \n",
    "    \n",
    "    else:\n",
    "        output2 = True\n",
    "\n",
    "    # Prepare data\n",
    "    ids = df[id_column].values\n",
    "    probabilities = df['Count'].values\n",
    "\n",
    "    # Normalize probabilities\n",
    "    probabilities = probabilities / probabilities.sum()\n",
    "\n",
    "    # Create the array of all possible ids with a count of 4\n",
    "    all_ids = np.repeat(ids, 4)\n",
    "\n",
    "    # Create the array of probabilities, boosted by the number of times each name is still available\n",
    "    probabilities = np.repeat(probabilities, 4)\n",
    "\n",
    "    result = []\n",
    "    for _ in range(n):\n",
    "        # Choose a name\n",
    "        chosen_index = np.random.choice(len(all_ids), p=probabilities/probabilities.sum())\n",
    "        chosen_name = all_ids[chosen_index]\n",
    "\n",
    "        # Append to result\n",
    "        result.append(chosen_name)\n",
    "\n",
    "        # Remove this name from the selection process\n",
    "        all_ids = np.delete(all_ids, chosen_index)\n",
    "        probabilities = np.delete(probabilities, chosen_index)\n",
    "\n",
    "\n",
    "    # Formatting output1\n",
    "    pd_result = pd.Series(result).value_counts().reset_index().rename(columns={'index':id_column,'count':'Count'})\n",
    "    pd_result['Year'] = year\n",
    "    pd_result['State'] = state\n",
    "\n",
    "    # Depending on id_1 or id_2...\n",
    "    if id_column == 'id_1':\n",
    "        pd_result[['Sex','Name','Year']] = pd_result[id_column].str.split(' \\| ',expand=True)\n",
    "        pd_result['id_2'] = pd_result['Sex'] + ' | ' + pd_result['Name']\n",
    "\n",
    "    else:\n",
    "        pd_result[['Sex','Name']] = pd_result[id_column].str.split(' \\| ',expand=True)\n",
    "        pd_result['id_1'] = pd_result['Sex'] + ' | ' + pd_result['Name'] + ' | ' + pd_result['Year'].astype(str)\n",
    "    \n",
    "    # Format output\n",
    "    output1 = pd_result[['State', 'Sex', 'Year', 'Name', 'Count', 'id_1', 'id_2']]\n",
    "\n",
    "    return output1, output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Generate masked names\n",
    "\n",
    "Here we generate the masked names and save them to the folder:\n",
    "'{root}/SupportingDocs/Names/02_Wrangled/StateFirstNames'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory that we'll drop files into\n",
    "dir_wrangled = '../../SupportingDocs/Names/02_Wrangled/StateFirstNames'\n",
    "os.makedirs(dir_wrangled, exist_ok=True)\n",
    "\n",
    "\n",
    "# Define bounds of our for-loop.  Only calculating for years within our vital stats known birth years\n",
    "year_start = df_vital_stats['Year'].min()\n",
    "year_end = df_vital_stats['Year'].max()\n",
    "\n",
    "# Loop through years of vital stats, checking the expected births vs. what we have name data for\n",
    "for cur_year in tqdm(np.arange(year_start,year_end)):\n",
    "    \n",
    "    # Using vital stats, find year and number of births\n",
    "    count_births = df_vital_stats.query(f'Year == {cur_year}')['Births'].iloc[0]\n",
    "\n",
    "    # Find number of births accounted for in state SSA naming data\n",
    "    df_state = df_state_names.query(f'Year == {cur_year}')\n",
    "    sum_state_names = df_state['Count'].sum()\n",
    "\n",
    "    # Filter down the national set to the current year first...\n",
    "    df_sub_national = df_national.query(f'Year == {cur_year}') \n",
    "\n",
    "    # Calculate number of masked names as described in documentation above\n",
    "    num_masked_names = count_births - sum_state_names\n",
    "\n",
    "    # If there's more than 0 masked names (there always should be...)\n",
    "    if num_masked_names > 0:\n",
    "\n",
    "        # Generate pool #1\n",
    "        pool1 = df_sub_national[~df_sub_national['id_1'].isin(df_state['id_1'])]\n",
    "\n",
    "        # Using our custom function, generate \"Masked Names\"\n",
    "        generated_masked_records, bool_ok = generate_names(pool1, num_masked_names, 'id_1', cur_year, state_abbreviation)\n",
    "\n",
    "        new_num_masked_names = num_masked_names - len(generated_masked_records)\n",
    "\n",
    "        # If second output of last record was false, we need to turn to another pool...\n",
    "        if bool_ok == False:\n",
    "            \n",
    "            # Need to perform 2 anti joins -> historic national records not in the state dataset AND not in the recently generated masked dataset\n",
    "            pool2_unfiltered = df_sub_national[~df_sub_national['id_2'].isin(df_state['id_2'])]\n",
    "            pool2 = pool2_unfiltered[~pool2_unfiltered['id_2'].isin(generated_masked_records['id_2'])]\n",
    "\n",
    "            # Using our custom function, generate \"Masked Names\"\n",
    "            generated_masked_records2, _ = generate_names(pool2, new_num_masked_names, 'id_2', cur_year, state_abbreviation)\n",
    "\n",
    "            # Redefine our output\n",
    "            generated_masked_records = pd.concat([generated_masked_records,generated_masked_records2])\n",
    "\n",
    "        # Concat our original state data (df_state) with our new masked records.  Drop columns unused \n",
    "        df = pd.concat([df_state,generated_masked_records]).drop(['id_1','id_2'],axis='columns')\n",
    "\n",
    "        # Write out to csv in save location\n",
    "        df.to_csv(f'../../SupportingDocs/Names/02_Wrangled/StateFirstNames/year{cur_year}.csv', index=False)\n",
    "\n",
    "    # Nothing should come down here...\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Reread data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of files we just downloaded to\n",
    "list_files = os.listdir(dir_wrangled)\n",
    "\n",
    "output = []\n",
    "# For each file, load and append to list we'll eventually concat together\n",
    "for file in list_files:\n",
    "    df = pd.read_csv(f'{dir_wrangled}/{file}')\n",
    "    output.append(df)\n",
    "\n",
    "# Concat all dataframes into one\n",
    "df_names_with_masks = pd.concat(output)\n",
    "\n",
    "# Drop state column, useful in files for reference that we're working state-level, but unimportant for computation\n",
    "df_names_with_masks = df_names_with_masks.drop('State', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Append Early dates\n",
    "\n",
    "We only have vital birthing stats (count births for each state) from 1914 moving forward.\n",
    "However, the distribution of first names is still important for years prior.\n",
    "\n",
    "We'll just use national-level breakdowns for years 1880-1914."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query national data for years prior to our compiled data above\n",
    "df_national_early_data = df_national.query(f'Year < {df_names_with_masks[\"Year\"].min()}')[['Sex', 'Year', 'Name', 'Count']]\n",
    "\n",
    "# Rename our final formatted data as df_names\n",
    "df_names = pd.concat([df_names_with_masks,df_national_early_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Combine with race data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.1 Read race data\n",
    "\n",
    "Gives breakdown of first names by race in the united states.\n",
    "For names not assessed, there is a general breakdown.\n",
    "* Data source [linked here](https://www.nature.com/articles/sdata201825) - description\n",
    "* Data source [linked here](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/TYJKEZ) - download location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "df_race = pd.read_excel(f'../../SupportingDocs/Names/01_Raw/firstnames.xlsx',\n",
    "                   sheet_name='Data')\n",
    "\n",
    "# Last row indicates probabilities for all other names\n",
    "row_allothernames = df_race.iloc[-1:]\n",
    "\n",
    "# Remove that last row from the data\n",
    "df_race = df_race.iloc[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5.2 Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data and drop columns we don't want to use\n",
    "df = pd.merge(df_names, df_race, left_on = 'Name', right_on = 'firstname', how='left')\\\n",
    "       .drop(['firstname','obs'] , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Aggregate Calculations\n",
    "\n",
    "We eventually want to see how likely any name is for any specific year/race/sex combination.\n",
    "To do this, we'll need to:\n",
    "1. (Numerator) Approximate how many people of a given race/sex have a specific name within a given year. \n",
    "2. (Denominator) Approximate how many people fit within a race/sex category within a given year for our dataset.\n",
    "3.  Calculate likelyhood by taking numerator / denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Melt our data\n",
    "\n",
    "We want things to be in a more easily accessible long format to perform some of our calculations.\n",
    "\n",
    "Note that when we melt, normalize, and start to do calculations, we are removing any information on the pct2race field.\n",
    "That means that our percents for the 5 race fields will NOT sum to 100%.  \n",
    "We don't intentionally bring in those percents / redistribute them intentionally.\n",
    "We're just reducing our dataset to people who only identify as having 1 race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define value variables we'll melt on\n",
    "perc_race_cols = ['pcthispanic','pctwhite','pctblack','pctapi','pctaian']\n",
    "\n",
    "# Melt our data so we only see one percent_race column per row.\n",
    "#### New data only has columns ['Name','Sex','Year','Count','variable','value']\n",
    "df_melted = pd.melt(df.reset_index(), id_vars=['Name','Sex','Year','Count'], value_vars=perc_race_cols)\n",
    "\n",
    "# Normalize to 1.0 instead of 100 for value variables\n",
    "df_melted['value'] = df_melted['value'] * 0.01\n",
    "\n",
    "# We only care about where the value (percent chance of having a race given a name) is more than 0\n",
    "df_melted = df_melted.query('value > 0')\n",
    "\n",
    "# Map the percentage column name with the race category it represents. NOT REPLACING VALUE HERE\n",
    "variable_mapping = {\n",
    "    'pcthispanic':'Hispanic',\n",
    "    'pctwhite':'White',\n",
    "    'pctblack':'Black or African American',\n",
    "    'pctapi':'Asian or Pacific Islander',\n",
    "    'pctaian':'American Indian or Alaska Native'\n",
    "}\n",
    "\n",
    "# Use our mapping to replace the variable name\n",
    "df_melted['variable'] = df_melted['variable'].replace(variable_mapping)\n",
    "\n",
    "# Rename our variable as race for readability/understandability moving forward\n",
    "df_melted.rename(columns={'variable':'Race'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calculate our \"Numerator\"\n",
    "\n",
    "Take the given example:\n",
    "* There are 100 instances of a Male named \"BEN\" in our dataset for the year 2010\n",
    "* Roughly 20% of all people named \"BEN\" are Asian \n",
    "\n",
    "Based on these 2 statements, we can assume that there are 20 Asians named BEN (in our dataset) for the year 2010.\n",
    "We'll use this logic to calculate our numerator in finding likelyhood of a name by race, sex, and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted['NameCount_Race'] = df_melted['Count'] * df_melted['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Calculate our \"Denominator\"\n",
    "\n",
    "Now take the example:\n",
    "\n",
    "* There are only 3 possible names for Asian males in 2010.\n",
    "   * 20 are named Ben\n",
    "   * 5 are named John\n",
    "   * 70 are named Jack\n",
    "\n",
    "How would you go about finding likelyhood a given name for Asian males in 2010?\n",
    "You'd find out how many total Asian males existed in our data for 2010: `20 + 5 + 70 = 95`.\n",
    "Then you'd take:\n",
    "   * `20 / 95 = 0.2105...` chance of being named Ben\n",
    "   * `5 / 95 = 0.05263...` chance of being named John\n",
    "   * `70 / 95 = 0.7368...` chance of being named Jack\n",
    "   \n",
    "In this section, we find the denominator, or the sum of all represented race/sex/year combos in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate that denominator using a groupby statement\n",
    "SumRaceCounts = df_melted.groupby(['Race','Year','Sex'])['NameCount_Race'].sum().reset_index()\\\n",
    "                         .rename(columns={'NameCount_Race':'Sum_RaceCount'})\n",
    "\n",
    "# Join back to our data\n",
    "df_melted = pd.merge(df_melted, SumRaceCounts, on=['Race','Year','Sex'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Calculate Name Probabilities\n",
    "\n",
    "We have the numerator, denominator.\n",
    "Now it's time to find our probability of having a name given the year, subject's race & sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted['Probability'] = df_melted['NameCount_Race'] / df_melted['Sum_RaceCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns of interest\n",
    "output_columns = ['Name','Sex','Year','Race','Probability']\n",
    "\n",
    "# Save our columns of interest\n",
    "df_melted[output_columns].to_csv(f'../../SupportingDocs/Names/03_Complete/firstname_probabilities.csv',\n",
    "                                header = True, index=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

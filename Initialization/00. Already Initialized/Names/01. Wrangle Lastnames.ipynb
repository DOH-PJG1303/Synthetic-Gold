{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangle Lastnames\n",
    "\n",
    "The purpose of this script is to wrangle data on last names.\n",
    "\n",
    "Data sources:\n",
    "* Census Surname data 2000 - https://api.census.gov/data/2000/surname (perform API to get data)\n",
    "* Census Surname data 2010 - https://api.census.gov/data/2010/surname (perform API to get data)\n",
    "* Census Surname data 1990 - https://www2.census.gov/topics/genealogy/1990surnames/dist.all.last\n",
    "\n",
    "In this script, we're looking to compile the likelyhood that given a specific race, you would have a specific last name.\n",
    "This information is only applied at the beginning of the simulation when we initialize our population.\n",
    "From that point forward, lastnames are passed down.\n",
    "\n",
    "----------------------\n",
    "\n",
    "<p>Author: PJ Gibson</p>\n",
    "<p>Date: 2023-01-10</p>\n",
    "<p>Updated Date: 2023-07-26</p>\n",
    "<p>Contact: peter.gibson@doh.wa.gov</p>\n",
    "<p>Other Contact: pjgibson25@gmail.com</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import libraries, functions, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Census Data 2000, 2010 via API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Define your API key\n",
    "\n",
    "You can request an API key at [this link](https://api.census.gov/data/key_signup.html).\n",
    "One will be sent to you via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the key from the pickle file\n",
    "with open('secrets.pkl', 'rb') as f:\n",
    "    census_key = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Create our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_census_surnames( year, key ):\n",
    "    '''\n",
    "    A query to collect census surname data for a specific decennial year (either 2000 or 2010)\n",
    "    \n",
    "    Args:\n",
    "        year - int, year to perform query for\n",
    "        key - str, census API key\n",
    "        \n",
    "    Returns:\n",
    "        pandas dataframe object reflecting data we'll use    \n",
    "    '''\n",
    "    # Define columns to query\n",
    "    census_requested_cols = ['NAME',\n",
    "                             'COUNT',\n",
    "                             'PROP100K',\n",
    "                             'PCTAPI',\n",
    "                             'PCTBLACK',\n",
    "                             'PCTAIAN',\n",
    "                             'PCTWHITE',\n",
    "                             'PCTHISPANIC',\n",
    "                             'PCT2PRACE']\n",
    "    \n",
    "    # Define base URL\n",
    "    census_url = f'https://api.census.gov/data/{year}/surname'\n",
    "\n",
    "    # Define api location\n",
    "    api_url = f'{census_url}?get={\",\".join(census_requested_cols)}&RANK=0:1000000&key={key}'\n",
    "    \n",
    "    # Perform API request\n",
    "    response_census = requests.get(api_url)\n",
    "\n",
    "    # Parse the API text output into pandas dataframe, rename the columns\n",
    "    df_census = pd.DataFrame(json.loads(response_census.text), dtype = str)\n",
    "    df_census.columns = census_requested_cols + ['RANK']\n",
    "    \n",
    "    # Remove row of just column names, where name = \"ALL OTHER NAMES\"\n",
    "    df_census = df_census[1:]\n",
    "    \n",
    "    # Replace (S) with null values\n",
    "    df_census.replace('(S)', np.nan, inplace=True)\n",
    "    \n",
    "    # Define float columns, int cols\n",
    "    float_cols = ['PROP100K','PCTAPI','PCTBLACK','PCTAIAN','PCTWHITE','PCTHISPANIC','PCT2PRACE']\n",
    "    int_cols = ['COUNT','RANK']\n",
    "    \n",
    "    # change type to floats\n",
    "    for column in float_cols:\n",
    "        df_census[column] = df_census[column].astype(float)\n",
    "    \n",
    "    # change type to ints\n",
    "    for column in int_cols:\n",
    "        df_census[column] = df_census[column].astype(int)\n",
    "    \n",
    "    # Return outputs\n",
    "    return df_census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Apply our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "census2000 = get_census_surnames( 2000, census_key )\n",
    "census2010 = get_census_surnames( 2010, census_key )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Census data 1990\n",
    "\n",
    "Also performed via API but not querying for specific variables, moreso webscraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL we'll scrape from\n",
    "census_url_1990 = 'https://www2.census.gov/topics/genealogy/1990surnames/dist.all.last'\n",
    "\n",
    "# API Call\n",
    "apiresponse_1990 = requests.get(census_url_1990)\n",
    "\n",
    "# Split the raw document string via newlines and convert to series\n",
    "precensus1990 = pd.Series(apiresponse_1990.text.split('\\n'))\n",
    "\n",
    "# Further split each line by the space characters (one or more, taking the max)\n",
    "census1990 = precensus1990.str.split('\\s+',expand=True)\n",
    "\n",
    "# Rename columns and dropna\n",
    "census1990.columns = ['NAME','FREQUENCY','CUM_FREQUENCY','RANK']\n",
    "census1990 = census1990.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Identify extra names\n",
    "\n",
    "There are several lastnames that aren't included in the 2010 dataset that are included in the 2000 and 1990 dataset.  \n",
    "Since the constrictions of the 2010 census data are that the lastname must occur 100 or more times, we'll assume that all of the names found in 1990/2000 still exist in 2010, they just occur under that 100 threshold.\n",
    "We'll assume that they occur 99 times.\n",
    "\n",
    "We already know the breakdowns by percent race for the 2000 data, but we'll have to infer the breakdowns by race for the 1990 data based on the 2010 row that says `Name == \"ALL OTHER NAMES\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Extranames 2000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out what was in the 2000 data but not the 2010 data\n",
    "extranames2000 = pd.merge(census2000, census2010.NAME, on='NAME', how='left', indicator=True)\\\n",
    "                   .query('_merge == \"left_only\"')\\\n",
    "                   .drop('_merge', axis=1)\n",
    "\n",
    "# Overwrite the \"COUNT\" field to read 99, just 1 below the threshold for making the dataset\n",
    "extranames2000['COUNT'] = 99\n",
    "extranames2000['PROP100K'] = np.nan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Othernames 1990 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out what was in the 1990 data but not the 2010 data\n",
    "extranames1990 = pd.merge(census1990, census2010.NAME, on='NAME', how='left', indicator=True)\\\n",
    "                   .query('_merge == \"left_only\"')\\\n",
    "                   .drop('_merge', axis=1)\n",
    "\n",
    "# Find out what was in the 1990 data not already in the \"othernames2000\" dataset\n",
    "extranames1990 = pd.merge(extranames1990, extranames2000.NAME, on='NAME', how='left', indicator=True)\\\n",
    "                   .query('_merge == \"left_only\"')\\\n",
    "                   .drop('_merge', axis=1)\n",
    "\n",
    "# Add rank == 0 bit for joining purposes\n",
    "extranames1990['RANK'] = 0\n",
    "\n",
    "# Join with our data of \"ALL OTHER NAMES\"\n",
    "extranames1990 = pd.merge(extranames1990, census2010.drop('NAME',axis=1), on='RANK', how='inner')\n",
    "\n",
    "# Make count = 99 and drop unused columns\n",
    "extranames1990['COUNT'] = 99\n",
    "extranames1990['PROP100K'] = np.nan\n",
    "extranames1990.drop(['FREQUENCY','CUM_FREQUENCY'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine and wrangle data\n",
    "\n",
    "\n",
    "We eventually want to see how likely any last name is for any specific race.\n",
    "To do this, we'll need to:\n",
    "\n",
    "1. (Numerator) Approximate how many people of a given race have each specific lastname.\n",
    "2. (Denominator) Approximate how many total people of each race within our dataset.\n",
    "3.  Calculate probability by taking numerator / denominator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Melt our data\n",
    "\n",
    "We want things to be in a more easily accessible long format to perform some of our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unionbyname our datasets\n",
    "df = pd.concat([census2010, extranames2000, extranames1990])\n",
    "\n",
    "# Remove the all other names field\n",
    "df = df.query('NAME != \"ALL OTHER NAMES\"')\n",
    "\n",
    "# Melt the data, dropna\n",
    "df_melted = pd.melt(df, id_vars=['NAME','COUNT'], value_vars=['PCTAPI','PCTBLACK','PCTAIAN','PCTWHITE','PCTHISPANIC'])\\\n",
    "              .dropna(subset=['value'])\n",
    "\n",
    "# Ensure we're only working with non-zero likelyhoods\n",
    "df_melted = df_melted.query('value > 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Clean melted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize our percent to 1.0 instead of 100\n",
    "df_melted['value'] = df_melted['value'] / 100\n",
    "\n",
    "\n",
    "# Map the percentage column name with the race category it represents. NOT REPLACING VALUE HERE\n",
    "variable_mapping = {\n",
    "    'PCTHISPANIC':'Hispanic',\n",
    "    'PCTWHITE':'White',\n",
    "    'PCTBLACK':'Black or African American',\n",
    "    'PCTAPI':'Asian or Pacific Islander',\n",
    "    'PCTAIAN':'American Indian or Alaska Native'\n",
    "}\n",
    "\n",
    "# Use our mapping to replace the variable name\n",
    "df_melted['variable'] = df_melted['variable'].replace(variable_mapping)\n",
    "df_melted.rename(columns={'variable':'Race',\n",
    "                          'NAME':'Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Calculate our \"Numerator\"\n",
    "\n",
    "Take the given example:\n",
    "* There are 100 instances of people with lastname \"SMITH\".\n",
    "* Roughly 20% of all people with lastname \"SMITH\" are Asian.\n",
    "\n",
    "Based on these 2 statements, we can assume that there are 20 Asians a lastname of \"SMITH\" (in our dataset).\n",
    "We'll use this logic to calculate our numerator in finding likelyhood of a name by race, sex, and year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new column\n",
    "df_melted['Race_NameCount'] = df_melted['COUNT'] * df_melted['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Calculate our \"Denominator\"\n",
    "\n",
    "Now take the example:\n",
    "\n",
    "* There are only 3 possible lastnames for Asian people (I know...super simplified)\n",
    "   * 20 with lastname \"SMITH\"\n",
    "   * 5 with lastname \"JACKSON\"\n",
    "   * 70 with lastname \"LEE\"\n",
    "\n",
    "How would you go about finding the probability any Asian might have a speicific lastname?\n",
    "You'd find out how many total Asian existed in our data: `20 + 5 + 70 = 95`.\n",
    "Then you'd take:\n",
    "   * `20 / 95 = 0.2105...` chance of having a lastname \"SMITH\"\n",
    "   * `5 / 95 = 0.05263...` chance of having a lastname \"JACKSON\"\n",
    "   * `70 / 95 = 0.7368...` chance of having a lastname \"LEE\"\n",
    "   \n",
    "In this section, we find the denominator, or the sum of all represented races in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate that denominator using a groupby statement\n",
    "SumRaceCounts = df_melted.groupby('Race')['Race_NameCount'].sum().reset_index()\\\n",
    "                         .rename(columns={'Race_NameCount':'Sum_RaceCount'})\n",
    "\n",
    "# Join back to our data\n",
    "df_melted = pd.merge(df_melted, SumRaceCounts, on='Race', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Calculate Name Probabilities\n",
    "\n",
    "We have the numerator, denominator.\n",
    "Now it's time to find our probability of having a name given a subject's race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted['Probability'] = df_melted['Race_NameCount'] / df_melted['Sum_RaceCount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output columns\n",
    "output_columns = ['Name','Race','Probability']\n",
    "\n",
    "# Save\n",
    "df_melted[output_columns].to_csv(f'../../../SupportingDocs/Names/03_Complete/lastname_probabilities.csv',\n",
    "                                header = True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
